{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install transformers torch scikit-learn","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:28:57.305917Z","iopub.execute_input":"2024-10-14T18:28:57.306759Z","iopub.status.idle":"2024-10-14T18:29:09.801938Z","shell.execute_reply.started":"2024-10-14T18:28:57.306701Z","shell.execute_reply":"2024-10-14T18:29:09.800768Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:29:50.466262Z","iopub.execute_input":"2024-10-14T18:29:50.466951Z","iopub.status.idle":"2024-10-14T18:29:51.801054Z","shell.execute_reply.started":"2024-10-14T18:29:50.466907Z","shell.execute_reply":"2024-10-14T18:29:51.799817Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Traceback (most recent call last):\n  File \"/opt/conda/bin/kaggle\", line 5, in <module>\n    from kaggle.cli import main\n  File \"/opt/conda/lib/python3.10/site-packages/kaggle/__init__.py\", line 7, in <module>\n    api.authenticate()\n  File \"/opt/conda/lib/python3.10/site-packages/kaggle/api/kaggle_api_extended.py\", line 407, in authenticate\n    raise IOError('Could not find {}. Make sure it\\'s located in'\nOSError: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport zipfile\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:30:00.177613Z","iopub.execute_input":"2024-10-14T18:30:00.178508Z","iopub.status.idle":"2024-10-14T18:30:19.500096Z","shell.execute_reply.started":"2024-10-14T18:30:00.178459Z","shell.execute_reply":"2024-10-14T18:30:19.499271Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"zip_file_path = '/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip'\n\n# Розпаковка ZIP-файлу\nwith zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n    zip_ref.extractall('/kaggle/working/')  # Розпаковуємо в робочу директорію\n\n# Перевірка, які файли були розпаковані\nprint(os.listdir('/kaggle/working/'))\n\n# Імпорт CSV файлу (припустимо, що в ZIP є файл train.csv)\ndf_train = pd.read_csv('/kaggle/working/train.csv')\n\n# Перегляд перших кількох рядків\nprint(df_train.head())","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:34:23.115926Z","iopub.execute_input":"2024-10-14T18:34:23.116786Z","iopub.status.idle":"2024-10-14T18:34:24.809623Z","shell.execute_reply.started":"2024-10-14T18:34:23.116744Z","shell.execute_reply":"2024-10-14T18:34:24.808568Z"},"trusted":true},"outputs":[{"name":"stdout","text":"['.virtual_documents', 'train.csv']\n                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \n0             0        0       0       0              0  \n1             0        0       0       0              0  \n2             0        0       0       0              0  \n3             0        0       0       0              0  \n4             0        0       0       0              0  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:34:32.591054Z","iopub.execute_input":"2024-10-14T18:34:32.592070Z","iopub.status.idle":"2024-10-14T18:34:32.647516Z","shell.execute_reply.started":"2024-10-14T18:34:32.592017Z","shell.execute_reply":"2024-10-14T18:34:32.646144Z"},"trusted":true},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 159571 entries, 0 to 159570\nData columns (total 8 columns):\n #   Column         Non-Null Count   Dtype \n---  ------         --------------   ----- \n 0   id             159571 non-null  object\n 1   comment_text   159571 non-null  object\n 2   toxic          159571 non-null  int64 \n 3   severe_toxic   159571 non-null  int64 \n 4   obscene        159571 non-null  int64 \n 5   threat         159571 non-null  int64 \n 6   insult         159571 non-null  int64 \n 7   identity_hate  159571 non-null  int64 \ndtypes: int64(6), object(2)\nmemory usage: 9.7+ MB\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:34:35.925271Z","iopub.execute_input":"2024-10-14T18:34:35.925672Z","iopub.status.idle":"2024-10-14T18:34:35.991816Z","shell.execute_reply.started":"2024-10-14T18:34:35.925630Z","shell.execute_reply":"2024-10-14T18:34:35.990148Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"               toxic   severe_toxic        obscene         threat  \\\ncount  159571.000000  159571.000000  159571.000000  159571.000000   \nmean        0.095844       0.009996       0.052948       0.002996   \nstd         0.294379       0.099477       0.223931       0.054650   \nmin         0.000000       0.000000       0.000000       0.000000   \n25%         0.000000       0.000000       0.000000       0.000000   \n50%         0.000000       0.000000       0.000000       0.000000   \n75%         0.000000       0.000000       0.000000       0.000000   \nmax         1.000000       1.000000       1.000000       1.000000   \n\n              insult  identity_hate  \ncount  159571.000000  159571.000000  \nmean        0.049364       0.008805  \nstd         0.216627       0.093420  \nmin         0.000000       0.000000  \n25%         0.000000       0.000000  \n50%         0.000000       0.000000  \n75%         0.000000       0.000000  \nmax         1.000000       1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>159571.000000</td>\n      <td>159571.000000</td>\n      <td>159571.000000</td>\n      <td>159571.000000</td>\n      <td>159571.000000</td>\n      <td>159571.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.095844</td>\n      <td>0.009996</td>\n      <td>0.052948</td>\n      <td>0.002996</td>\n      <td>0.049364</td>\n      <td>0.008805</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.294379</td>\n      <td>0.099477</td>\n      <td>0.223931</td>\n      <td>0.054650</td>\n      <td>0.216627</td>\n      <td>0.093420</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df_train['labels'] = df_train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values.tolist()\n\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df_train['comment_text'], df_train['labels'], test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:38:00.191108Z","iopub.execute_input":"2024-10-14T18:38:00.191968Z","iopub.status.idle":"2024-10-14T18:38:00.614894Z","shell.execute_reply.started":"2024-10-14T18:38:00.191924Z","shell.execute_reply":"2024-10-14T18:38:00.613975Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class ToxicCommentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.float)\n        }\n\n# Ініціалізація токенізатора BERT\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Створення тренувальних і тестових наборів\ntrain_dataset = ToxicCommentDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\ntest_dataset = ToxicCommentDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:38:04.482624Z","iopub.execute_input":"2024-10-14T18:38:04.483485Z","iopub.status.idle":"2024-10-14T18:38:07.805481Z","shell.execute_reply.started":"2024-10-14T18:38:04.483446Z","shell.execute_reply":"2024-10-14T18:38:07.804755Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70bc910e8823400095f15fe75bf92268"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f47824e4fcbc4611aa5381ceb767cd9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"610661fe18d74cc09a567ccc75947bdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe0b5d53bbe54cbab021002daed19c96"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)  # 6 - кількість класів","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:38:11.754564Z","iopub.execute_input":"2024-10-14T18:38:11.754956Z","iopub.status.idle":"2024-10-14T18:38:15.923133Z","shell.execute_reply.started":"2024-10-14T18:38:11.754918Z","shell.execute_reply":"2024-10-14T18:38:15.922198Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0267a5bfabf34f76b8e939ee220dac83"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    warmup_steps=200,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    eval_strategy=\"steps\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:38:32.873568Z","iopub.execute_input":"2024-10-14T18:38:32.874490Z","iopub.status.idle":"2024-10-14T18:38:32.917001Z","shell.execute_reply.started":"2024-10-14T18:38:32.874445Z","shell.execute_reply":"2024-10-14T18:38:32.916149Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T18:38:38.184594Z","iopub.execute_input":"2024-10-14T18:38:38.184987Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111353585555561, max=1.0)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"127d57d40e1042fcbb7d42112218c80e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241014_183851-7n57ubqd</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/priorat-ceon-goit-global/huggingface/runs/7n57ubqd' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/priorat-ceon-goit-global/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/priorat-ceon-goit-global/huggingface' target=\"_blank\">https://wandb.ai/priorat-ceon-goit-global/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/priorat-ceon-goit-global/huggingface/runs/7n57ubqd' target=\"_blank\">https://wandb.ai/priorat-ceon-goit-global/huggingface/runs/7n57ubqd</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='691' max='5985' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 691/5985 11:45:30 < 90:20:53, 0.02 it/s, Epoch 0.35/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.618700</td>\n      <td>0.605584</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.585600</td>\n      <td>0.555244</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.528000</td>\n      <td>0.478882</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.440400</td>\n      <td>0.371745</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.330300</td>\n      <td>0.274408</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.249900</td>\n      <td>0.211690</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.200000</td>\n      <td>0.176664</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.164200</td>\n      <td>0.149221</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.142300</td>\n      <td>0.124826</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.110700</td>\n      <td>0.103418</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.093600</td>\n      <td>0.091082</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.076400</td>\n      <td>0.080660</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.082900</td>\n      <td>0.079835</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.085900</td>\n      <td>0.066814</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.060600</td>\n      <td>0.062896</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.064400</td>\n      <td>0.062594</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.061500</td>\n      <td>0.057782</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.065100</td>\n      <td>0.059030</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.053900</td>\n      <td>0.061667</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.066000</td>\n      <td>0.053826</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.057900</td>\n      <td>0.060661</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.066600</td>\n      <td>0.057828</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.058000</td>\n      <td>0.054311</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.051000</td>\n      <td>0.054686</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.055000</td>\n      <td>0.050606</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.050800</td>\n      <td>0.051029</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.056800</td>\n      <td>0.058206</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.052100</td>\n      <td>0.048879</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.051500</td>\n      <td>0.048389</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.049600</td>\n      <td>0.049256</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.058500</td>\n      <td>0.049231</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.044200</td>\n      <td>0.047475</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.069200</td>\n      <td>0.053466</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.056300</td>\n      <td>0.048866</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.060900</td>\n      <td>0.047478</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.055600</td>\n      <td>0.046686</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.052000</td>\n      <td>0.056917</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.052200</td>\n      <td>0.048383</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.058800</td>\n      <td>0.048913</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.053000</td>\n      <td>0.048060</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.048300</td>\n      <td>0.047900</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.057000</td>\n      <td>0.046560</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.049300</td>\n      <td>0.048468</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.049300</td>\n      <td>0.047399</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.041500</td>\n      <td>0.045772</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.049500</td>\n      <td>0.048242</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.052000</td>\n      <td>0.049302</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.045100</td>\n      <td>0.046718</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.052000</td>\n      <td>0.048177</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.039600</td>\n      <td>0.044846</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.044800</td>\n      <td>0.045881</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.047400</td>\n      <td>0.046960</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.051000</td>\n      <td>0.050083</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.047700</td>\n      <td>0.044219</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.037600</td>\n      <td>0.046674</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.048100</td>\n      <td>0.043943</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.042000</td>\n      <td>0.044326</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.047300</td>\n      <td>0.046830</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.037200</td>\n      <td>0.045669</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.041500</td>\n      <td>0.044533</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.041300</td>\n      <td>0.042984</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.043200</td>\n      <td>0.043359</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.045400</td>\n      <td>0.044517</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.044200</td>\n      <td>0.043815</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.040900</td>\n      <td>0.044087</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.042900</td>\n      <td>0.043172</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.049200</td>\n      <td>0.044633</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.040300</td>\n      <td>0.044115</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='211' max='499' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [211/499 04:08 < 05:41, 0.84 it/s]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"trainer.save_model('./best_model')\ntokenizer.save_pretrained('./best_model')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.evaluate()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n    outputs = model(**inputs)\n    predictions = torch.sigmoid(outputs.logits)\n    return predictions\n\n# Приклад передбачення\npredict(\"This is a very bad comment!\")\npredict(\"I will kill you\")\npredict(\"god hate all jews\")","metadata":{},"outputs":[],"execution_count":null}]}